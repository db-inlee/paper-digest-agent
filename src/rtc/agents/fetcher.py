"""CandidateFetcher ì—ì´ì „íŠ¸ - ë…¼ë¬¸ ìˆ˜ì§‘ (ë¹„-LLM)."""

import logging
import re
from dataclasses import dataclass

import arxiv

from rtc.agents.base import BaseAgent
from rtc.config import get_settings
from rtc.mcp.servers.hf_papers_server import HFPapersServer, paper_dict_to_candidate
from rtc.schemas import PaperCandidate

logger = logging.getLogger(__name__)


@dataclass
class FetchInput:
    """Fetcher ì…ë ¥."""

    run_date: str  # YYYY-MM-DD


@dataclass
class FetchOutput:
    """Fetcher ì¶œë ¥."""

    candidates: list[PaperCandidate]
    total_collected: int
    total_after_filter: int
    skipped_previously_processed: int
    skipped_keyword_filter: int
    skipped_venue_filter: int = 0
    venue_enriched_count: int = 0


class CandidateFetcher(BaseAgent[FetchInput, FetchOutput]):
    """HF Papersì—ì„œ ë…¼ë¬¸ ìˆ˜ì§‘ ë° í•„í„°ë§ (ë¹„-LLM).

    ê¸°ì¡´ collect_node + filter_node ë¡œì§ì„ í†µí•©í•©ë‹ˆë‹¤.
    """

    name = "candidate_fetcher"
    uses_llm = False

    def __init__(self):
        self.settings = get_settings()

    async def run(self, input: FetchInput) -> FetchOutput:
        """ë…¼ë¬¸ ìˆ˜ì§‘ ë° í•„í„°ë§ ì‹¤í–‰.

        Args:
            input: ì‹¤í–‰ ë‚ ì§œ ì •ë³´

        Returns:
            í•„í„°ë§ëœ ë…¼ë¬¸ í›„ë³´ ëª©ë¡
        """
        # 1. HF Papersì—ì„œ ìˆ˜ì§‘
        candidates = await self._collect_papers()
        total_collected = len(candidates)

        # 2. arXiv comment ë³´ê°• (venue ê°ì§€ìš©)
        venue_enriched = 0
        if self.settings.venue_filter_enabled:
            venue_enriched = self._enrich_arxiv_comments(candidates)

        # 3. í•„í„°ë§ ì ìš©
        filtered, stats = self._apply_filters(candidates)

        return FetchOutput(
            candidates=filtered,
            total_collected=total_collected,
            total_after_filter=len(filtered),
            skipped_previously_processed=stats["skipped_processed"],
            skipped_keyword_filter=stats["skipped_keyword"],
            skipped_venue_filter=stats.get("skipped_venue", 0),
            venue_enriched_count=venue_enriched,
        )

    async def _collect_papers(self) -> list[PaperCandidate]:
        """HF Papersì—ì„œ ë…¼ë¬¸ ìˆ˜ì§‘."""
        server = HFPapersServer()

        try:
            paper_dicts = await server.search_papers(
                days_back=self.settings.hf_papers_lookback_days,
                min_votes=self.settings.hf_papers_min_votes,
            )
            return [paper_dict_to_candidate(p) for p in paper_dicts]
        finally:
            await server.close()

    def _apply_filters(
        self, candidates: list[PaperCandidate]
    ) -> tuple[list[PaperCandidate], dict]:
        """í•„í„°ë§ ì ìš©.

        Args:
            candidates: ì›ë³¸ ë…¼ë¬¸ ëª©ë¡

        Returns:
            (í•„í„°ë§ëœ ëª©ë¡, í†µê³„ dict)
        """
        seen_ids: set[str] = set()
        filtered: list[PaperCandidate] = []

        # ì´ì „ì— ì²˜ë¦¬ëœ ë…¼ë¬¸ ID ë¡œë“œ
        processed_ids = self._load_processed_arxiv_ids()

        stats = {
            "skipped_processed": 0,
            "skipped_keyword": 0,
            "skipped_duplicate": 0,
            "skipped_hard_filter": 0,
            "skipped_venue": 0,
        }

        keywords = self.settings.get_effective_hf_keywords()

        for paper in candidates:
            base_id = paper.arxiv_id.split("v")[0]

            # ì´ì „ì— ì²˜ë¦¬ëœ ë…¼ë¬¸ ê±´ë„ˆë›°ê¸°
            if base_id in processed_ids:
                stats["skipped_processed"] += 1
                continue

            # ì¤‘ë³µ ì œê±°
            if base_id in seen_ids:
                stats["skipped_duplicate"] += 1
                continue
            seen_ids.add(base_id)

            # í•˜ë“œ í•„í„°
            if not self._passes_hard_filters(paper):
                stats["skipped_hard_filter"] += 1
                continue

            # í‚¤ì›Œë“œ í•„í„°
            if keywords and not self._matches_keywords(paper, keywords):
                stats["skipped_keyword"] += 1
                continue

            # Venue í•„í„°
            if self.settings.venue_filter_enabled:
                if self.settings.venue_filter_mode == "only" and paper.venue is None:
                    stats["skipped_venue"] += 1
                    continue
                if self.settings.venue_filter_mode == "boost" and paper.venue:
                    paper.matched_keywords.append(f"ğŸ“{paper.venue}")

            filtered.append(paper)

        return filtered, stats

    def _load_processed_arxiv_ids(self) -> set[str]:
        """ì´ì „ì— ì²˜ë¦¬ëœ ë…¼ë¬¸ ID ë¡œë“œ (reports í´ë” ê¸°ë°˜)."""
        processed_ids: set[str] = set()

        # reports/ í´ë”ì—ì„œ ì²˜ë¦¬ëœ ë…¼ë¬¸ ID ì¶”ì¶œ
        reports_dir = self.settings.reports_dir
        if not reports_dir.exists():
            return processed_ids

        for paper_dir in reports_dir.iterdir():
            if paper_dir.is_dir() and paper_dir.name != "daily":
                # í´ë”ëª…ì—ì„œ arxiv_id ì¶”ì¶œ (ì˜ˆ: 2601.20833-paper-title)
                parts = paper_dir.name.split("-")
                if parts:
                    arxiv_id = parts[0]
                    base_id = arxiv_id.split("v")[0]
                    processed_ids.add(base_id)

        return processed_ids

    def _passes_hard_filters(self, paper: PaperCandidate) -> bool:
        """í•˜ë“œ í•„í„° ì ìš©."""
        # ì´ˆë¡ 100ì ì´ìƒ í•„ìˆ˜
        if not paper.abstract or len(paper.abstract) < 100:
            return False

        # ì œëª© í•„ìˆ˜
        if not paper.title:
            return False

        # survey/tutorial ë“± ì œì™¸
        title_lower = paper.title.lower()
        skip_patterns = [
            "survey",
            "tutorial",
            "workshop report",
            "competition",
            "challenge report",
        ]

        for pattern in skip_patterns:
            if pattern in title_lower:
                return False

        return True

    def _matches_keywords(self, paper: PaperCandidate, keywords: list[str]) -> bool:
        """í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸. ë§¤ì¹­ëœ í‚¤ì›Œë“œë¥¼ paperì— ê¸°ë¡."""
        if not keywords:
            return True

        text = f"{paper.title} {paper.abstract}".lower()
        matched = [kw for kw in keywords if kw.lower() in text]
        if matched:
            paper.matched_keywords = matched
            return True
        return False

    def _enrich_arxiv_comments(self, candidates: list[PaperCandidate]) -> int:
        """arXiv APIë¡œ comment í•„ë“œ ë³´ê°• ë° venue ê°ì§€.

        commentê°€ ì—†ëŠ” ë…¼ë¬¸ë§Œ ëŒ€ìƒìœ¼ë¡œ batch lookup ìˆ˜í–‰.

        Returns:
            venueê°€ ê°ì§€ëœ ë…¼ë¬¸ ìˆ˜
        """
        # commentê°€ ì—†ëŠ” ë…¼ë¬¸ì˜ arxiv_id ìˆ˜ì§‘
        needs_comment = {
            p.arxiv_id: p for p in candidates if p.comment is None
        }
        if not needs_comment:
            return 0

        id_list = list(needs_comment.keys())
        logger.info("arXiv comment ë³´ê°•: %dê±´ ì¡°íšŒ", len(id_list))

        try:
            client = arxiv.Client()
            search = arxiv.Search(id_list=id_list)
            results = list(client.results(search))
        except Exception:
            logger.warning("arXiv API ì¡°íšŒ ì‹¤íŒ¨, comment ë³´ê°• ê±´ë„ˆëœ€", exc_info=True)
            return 0

        venue_count = 0
        for result in results:
            arxiv_id = result.entry_id.split("/abs/")[-1]
            # ë²„ì „ ì œê±°í•˜ì—¬ ë§¤ì¹­ (ì˜ˆ: 2401.12345v2 -> 2401.12345)
            base_id = arxiv_id.split("v")[0]

            paper = needs_comment.get(arxiv_id) or needs_comment.get(base_id)
            if paper is None:
                continue

            if result.comment:
                paper.comment = result.comment
                venue = self._extract_venue(result.comment)
                if venue:
                    paper.venue = venue
                    venue_count += 1

        logger.info("arXiv comment ë³´ê°• ì™„ë£Œ: %dê±´ ì¤‘ %dê±´ venue ê°ì§€", len(results), venue_count)
        return venue_count

    def _extract_venue(self, comment: str) -> str | None:
        """arXiv commentì—ì„œ í•™íšŒëª… ì¶”ì¶œ.

        ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ë§¤ì¹­:
        1. ì •ê·œì‹: "Accepted at NeurIPS 2025" ë“±ì˜ íŒ¨í„´
        2. ì§ì ‘ ë§¤ì¹­: í•™íšŒëª…ì´ commentì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        """
        conferences = self.settings.venue_filter_conferences

        # 1. ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹­
        pattern = r"(?:accepted|published|appearing|to appear)\s+(?:at|in|by)\s+(\w+)"
        match = re.search(pattern, comment, re.IGNORECASE)
        if match:
            detected = match.group(1)
            for conf in conferences:
                if detected.upper() == conf.upper():
                    return conf

        # 2. ì§ì ‘ ë§¤ì¹­ (comment ë‚´ì— í•™íšŒëª…ì´ í¬í•¨ëœ ê²½ìš°)
        comment_upper = comment.upper()
        for conf in conferences:
            if conf.upper() in comment_upper:
                return conf

        return None
